{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOIa4ATZu1my",
        "outputId": "cc8c1248-e323-43d8-d534-76b6c7929e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 30 00:13:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "TuC8RKmVvcZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae11d872-da9f-42f3-9b78-747b04e5128b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/MyDrive/essay_data/raw_data_transformer ./\n",
        "!cp -r /content/gdrive/MyDrive/essay_data/embeddings/ ./"
      ],
      "metadata": {
        "id": "RsF_4VJcxUKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GildS8-MN3B"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab, build_vocab_from_iterator\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "import re \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "url_base = 'raw_data_transformer'\n",
        "train_urls = ('train.de', 'train.de')\n",
        "val_urls = ('val.de', 'val.en')\n",
        "test_urls = ('test.de', 'test.en')\n",
        "\n",
        "embed_url = (\"embeds_de\", \"embeds_en\")\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "TRAIN_SIZE = 2000000\n",
        "VAL_SIZE = 2737 \n",
        "TEST_SIZE = 2169\n",
        "\n",
        "\n",
        "train_filepaths = [os.path.join(url_base, url) for url in train_urls]\n",
        "val_filepaths = [os.path.join(url_base, url) for url in val_urls]\n",
        "test_filepaths = [os.path.join(url_base, url) for url in test_urls]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLux09wvilGz"
      },
      "source": [
        "## Load Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPTsqXeMQXYg"
      },
      "outputs": [],
      "source": [
        "def add_special(token, embeds):\n",
        "  #insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
        "  token[0] = '<unk>'\n",
        "  token = np.insert(token, 1, '<pad>')\n",
        "  token = np.insert(token, 2, '<bos>')\n",
        "  token = np.insert(token, 3, '<eos>')\n",
        "\n",
        "  pad_emb = np.zeros((1,embeds.shape[1]))           #embedding for '<pad>' token. full 0\n",
        "  bos_emb = np.full((1,embeds.shape[1]), -1)        #embedding for '<bos>' token. full -1\n",
        "  eos_emb = np.full((1,embeds.shape[1]), 1)         #embedding for '<eos>' token. full 1\n",
        "\n",
        "  #insert embeddings for unk, pad, bos and eos tokens at top of embs.\n",
        "  # embs = np.vstack((pad_emb,bos_emb, eos_emb, embeds))\n",
        "  embeds = np.insert(embeds, 1, pad_emb, 0)\n",
        "  embeds = np.insert(embeds, 2, bos_emb, 0)\n",
        "  embeds = np.insert(embeds, 3, eos_emb, 0)\n",
        "  return token, embeds\n",
        "\n",
        "def load_pretrained_embeds(filename):\n",
        "  token,embeddings = [],[]\n",
        "  with open(filename,'rt', encoding='utf-8', errors='ignore') as fi:\n",
        "      full_content = fi.read().strip().split('\\n')\n",
        "  for i in range(len(full_content)):\n",
        "      i_word = full_content[i].split(' ')[0]\n",
        "      i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "      token.append(i_word)\n",
        "      embeddings.append(i_embeddings)\n",
        "  return add_special(np.array(token), np.array(embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y58ezxIymtRi"
      },
      "outputs": [],
      "source": [
        "# Place-holders\n",
        "embed = {\n",
        "    'de': [],\n",
        "    'en': []\n",
        "}\n",
        "token = {\n",
        "    'de': [],\n",
        "    'en': []\n",
        "}\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "def get_token(lang):\n",
        "  filename = f'raw_data_transformer/vocab.{lang}'\n",
        "  with open(filename, encoding='utf-8', errors='ignore') as f:\n",
        "    for line in f:\n",
        "      token[lang].append(line.strip())\n",
        "  token[lang].insert(PAD_IDX, special_symbols[PAD_IDX])\n",
        "  token[lang][BOS_IDX] = special_symbols[BOS_IDX]\n",
        "  token[lang][EOS_IDX] = special_symbols[EOS_IDX]\n",
        "\n",
        "def build_vocab(token):\n",
        "  vocab = {}\n",
        "  for i in range(len(token)):\n",
        "    vocab[token[i]] = i + 1\n",
        "  return vocab\n",
        "\n",
        "vocab_transform = {}\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  filepath = os.path.join('embeddings', f'embeds_{ln}')\n",
        "  token[ln], embed[ln] = load_pretrained_embeds(filepath)\n",
        "  # get_token(ln)\n",
        "  vocab_transform[ln] = vocab(build_vocab(token[ln]))\n",
        "  \n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3ZsYyXx6b8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a493003-5858-4492-bc5d-f35e332cc81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_data_transformer/train.de\n",
            "raw_data_transformer/val.de\n",
            "raw_data_transformer/test.de\n"
          ]
        }
      ],
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  counter = 0\n",
        "  print(filepaths[0])\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor_ = torch.tensor([vocab_transform['de'][token] for token in raw_de.strip().split()], dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor([vocab_transform['en'][token] for token in raw_en.strip().split()], dtype=torch.long)\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "    counter+=1\n",
        "    if counter == TRAIN_SIZE:\n",
        "      break\n",
        "  return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnnNRKOo7l1J"
      },
      "source": [
        "\n",
        "Language Translation with nn.Transformer and torchtext\n",
        "======================================================\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer. \n",
        "    - Use tochtext library to access  `Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>`__ dataset to train a German to English translation model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF4xivJX7l1L"
      },
      "source": [
        "Data Sourcing and Processing\n",
        "----------------------------\n",
        "\n",
        "`torchtext library <https://pytorch.org/text/stable/>`__ has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to use torchtext's inbuilt datasets, \n",
        "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "`Multi30k dataset from torchtext library <https://pytorch.org/text/stable/datasets.html#multi30k>`__\n",
        "that yields a pair of source-target raw sentences. \n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX1wqLBa7l1M"
      },
      "source": [
        "Seq2Seq Network using Transformer\n",
        "---------------------------------\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
        "need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
        "paper for solving machine translation tasks. \n",
        "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "encodings to provide position information of input tokens to the model. The second part is the \n",
        "actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model. \n",
        "Finally, the output of Transformer model is passed through linear layer\n",
        "that give un-normalized probabilities for each token in the target language. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDZDdu_17l1M"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# DEVICE = xm.xla_device()\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size, lang):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        # load embedding here\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size).from_pretrained(torch.FloatTensor(embed[lang]))\n",
        "        # self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network \n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size, SRC_LANGUAGE)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size, TGT_LANGUAGE)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5qUMJJ07l1N"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent model to look into\n",
        "the future words when making predictions. We will also need masks to hide\n",
        "source and target padding tokens. Below, let's define a function that will take care of both. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drNT7xgP7l1O"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkmwHyCL7l1O"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
        "define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hT3U8tU7l1P"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 300\n",
        "NHEAD = 10\n",
        "FFN_HID_DIM = 600\n",
        "BATCH_SIZE = 64\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "RESTORE = True\n",
        "FIRST_EPOCH = 1\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "if RESTORE == True:\n",
        "  FIRST_EPOCH = int(os.listdir('/content/gdrive/MyDrive/essay_data/checkpoint_transformer')[0].split('_')[1]) + 1\n",
        "  transformer.load_state_dict(torch.load(f'/content/gdrive/MyDrive/essay_data/checkpoint_transformer/checkpoint_{FIRST_EPOCH - 1}'))\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDV23pES7l1P"
      },
      "source": [
        "Collation\n",
        "---------\n",
        "\n",
        "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
        "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
        "defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
        "can be fed directly into our model.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umFU0vSm7l1Q"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import List\n",
        "\n",
        "PAD_IDX = vocab_transform['de']['<pad>']\n",
        "BOS_IDX = vocab_transform['de']['<bos>']\n",
        "EOS_IDX = vocab_transform['de']['<eos>']\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(lambda sent : sent.strip().split(), #Tokenization\n",
        "                                               vocab_transform[ln],                #Numericalization\n",
        "                                               tensor_transform)                   # Add BOS/EOS and create tensor\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auZMeMrA7l1Q"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each \n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvjKqbNH7l1Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "    \n",
        "    num_batch = 0\n",
        "    for src, tgt in train_dataloader:\n",
        "        num_batch += 1        \n",
        "        if num_batch % 100 == 0:\n",
        "          print(f'train batch: {num_batch}')\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss\n",
        "\n",
        "    return losses / num_batch\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "    num_batch = 0\n",
        "    for src, tgt in val_dataloader:\n",
        "        num_batch += 1\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / num_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate_raw(model: torch.nn.Module, src_sentence: torch.Tensor):\n",
        "    model.eval()\n",
        "    src = src_sentence\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 50, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval() \n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 50, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "def eval_bleu(model):\n",
        "  scores = 0\n",
        "  num_sent = 0\n",
        "\n",
        "  val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "  model.eval()\n",
        "  for inputs_batch, targets_batch in val_dataloader:\n",
        "    for i in range(inputs_batch.size(1)):\n",
        "      num_sent += 1\n",
        "      inputs, targets = inputs_batch[:,i:i+1], targets_batch[:,i:i+1]\n",
        "      \n",
        "      target_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(targets.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
        "      \n",
        "      pred_sentence = translate_raw(model, inputs)\n",
        "      if num_sent % 100 == 0:\n",
        "        print(f'valuation - target sentence {num_sent}: {target_sentence}')\n",
        "        print(f'valuation - target sentence {num_sent}: {pred_sentence}')\n",
        "\n",
        "      score = bleu_score([pred_sentence.split()], [[target_sentence.split()]])\n",
        "      scores += score \n",
        "  return scores / num_sent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(FIRST_EPOCH, NUM_EPOCHS+1):\n",
        "    print(f'start epoch: {epoch}')\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, blue score = {eval_bleu(transformer):.4f},\"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "    # save checkpoint\n",
        "    torch.save(transformer.state_dict(), f'checkpoint_{epoch}')\n",
        "    !rm -rf /content/gdrive/MyDrive/essay_data/checkpoint_transformer/*\n",
        "    !cp checkpoint_{epoch} /content/gdrive/MyDrive/essay_data/checkpoint_transformer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0EQgaaV-_sw",
        "outputId": "f5bc0375-8d40-41f8-ae1c-c2e23333499c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch: 4\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  An ambulance team brought the injured man to the clinic for medical treatment .                                           \n",
            "valuation - target sentence 100:  Ein <unk> <unk> den <unk> zur <unk> <unk> ins <unk> . \n",
            "valuation - target sentence 200:  But when they arrive , they receive a true heroes &apos; welcome : a feast , singing and dancing , and the best accommodations the community can offer .                            \n",
            "valuation - target sentence 200:  <unk> wenn sie <unk> , werden sie wie <unk> <unk> : ein <unk> , <unk> und <unk> und die beste <unk> , die der Ort zu bieten hat . \n",
            "valuation - target sentence 300:  Doctors couldn &apos;t be sure if he had a large <unk> or a small penis and were convinced he could never live a &quot; satisfactory life &quot; as a man .                            \n",
            "valuation - target sentence 300:  Die <unk> waren sich nicht <unk> <unk> , ob er eine große <unk> oder einen kleinen <unk> hatte und waren <unk> , er <unk> <unk> ein „ <unk> Leben “ als Mann <unk> . \n",
            "valuation - target sentence 400:  The system is fitted with coloured LEDs , which are bright enough that drivers can easily see the lights , even when the sun is low in the sky .                 \n",
            "valuation - target sentence 400:  Die Anlage ist mit <unk> <unk> ausgestattet , die so <unk> <unk> , dass die <unk> von den <unk> <unk> auch bei <unk> Sonne gut zu <unk> sind . \n",
            "valuation - target sentence 500:  They are a great team together .                                                                 \n",
            "valuation - target sentence 500:  <unk> sind sie ein <unk> Team . \n",
            "valuation - target sentence 600:  <unk> &apos;s daughter Katie was born with male chromosomes , but has a <unk> called complete <unk> <unk> syndrome .                                       \n",
            "valuation - target sentence 600:  <unk> , die <unk> von <unk> , wurde mit <unk> <unk> <unk> , hat aber eine <unk> <unk> <unk> <unk> . \n",
            "valuation - target sentence 700:  On July 25 , the U.N. estimated 100,000 have died in the conflict since March 2011 .                                                   \n",
            "valuation - target sentence 700:  Am <unk> Juli <unk> die UN die <unk> der <unk> in diesem <unk> seit März 2011 auf 100.000 . \n",
            "valuation - target sentence 800:  The situation hasn &apos;t changed much since <unk> &apos;s father passed away in 2002 .                                                     \n",
            "valuation - target sentence 800:  Die Situation hat sich kaum <unk> , seit <unk> <unk> 2002 <unk> . \n",
            "valuation - target sentence 900:  There may also be deviations that could show physicians the path to a new theory , under the umbrella of which all known natural laws could be united .                              \n",
            "valuation - target sentence 900:  <unk> finden sich auch <unk> , die den <unk> den Weg zu einer neuen Theorie <unk> <unk> , unter deren <unk> sich alle <unk> <unk> <unk> lassen . \n",
            "valuation - target sentence 1000:  Anyone visiting Cologne cannot miss the cathedral .                                                   \n",
            "valuation - target sentence 1000:  Wer Köln <unk> , kommt am Dom nicht <unk> . \n",
            "valuation - target sentence 1100:  Earlier this week , the inspectors said they had completed their first round of verification work , visiting 21 of 23 sites declared by Damascus .                         \n",
            "valuation - target sentence 1100:  <unk> der Woche hatten die <unk> <unk> , sie <unk> die erste <unk> der <unk> <unk> und 21 von 23 der von <unk> <unk> <unk> <unk> . \n",
            "valuation - target sentence 1200:  In the past the Pope decided everything on his own .                                       \n",
            "valuation - target sentence 1200:  In der <unk> <unk> der <unk> alles <unk> . \n",
            "valuation - target sentence 1300:  The stop ##AT##-##AT## and ##AT##-##AT## <unk> tactic has been criticized by a number of civil rights advocates .                                  \n",
            "valuation - target sentence 1300:  Die Stop ##AT##-##AT## and ##AT##-##AT## <unk> ##AT##-##AT## <unk> wurde von einer <unk> von <unk> <unk> . \n",
            "valuation - target sentence 1400:  For the training exercise , department commander , Hans <unk> , chose the <unk> premises on <unk> Strasse .                                                                         \n",
            "valuation - target sentence 1400:  <unk> Hans <unk> hatte sich in seiner <unk> das <unk> <unk> in der <unk> Straße <unk> . \n",
            "valuation - target sentence 1500:  He said European leaders need to be honest with their own people about the kind of espionage programs they &apos;ve used for years themselves .                      \n",
            "valuation - target sentence 1500:  Er <unk> , die <unk> <unk> <unk> ihrer <unk> gegenüber <unk> über die Art von <unk> sein , die sie selbst seit Jahren <unk> . \n",
            "valuation - target sentence 1600:  To that end , we commissioned the largest and most comprehensive poll of British business leaders asking them for their thoughts on Britain , business and the EU .                              \n",
            "valuation - target sentence 1600:  Zu diesem <unk> haben wir die <unk> und <unk> <unk> unter <unk> <unk> in <unk> <unk> und sie <unk> , was sie über <unk> , die Wirtschaft und die EU <unk> . \n",
            "valuation - target sentence 1700:  The tunnel , which <unk> the length of nearly six football pitches , links warehouses near <unk> , Mexico and San Diego , USA .                               \n",
            "valuation - target sentence 1700:  Der Tunnel , der im <unk> über eine <unk> von <unk> <unk> <unk> <unk> , <unk> <unk> in der Nähe von <unk> , <unk> , und San Diego , USA . \n",
            "valuation - target sentence 1800:  A young team of visionaries was headed for the White House , and the nation was ready for change .                                                     \n",
            "valuation - target sentence 1800:  Ein <unk> Team von <unk> war auf dem Weg ins <unk> Haus und die Nation war <unk> für <unk> . \n",
            "valuation - target sentence 1900:  The winners of the team and individual contests receive prizes .                                              \n",
            "valuation - target sentence 1900:  Die <unk> des <unk> und <unk> erhalten Preise . \n",
            "valuation - target sentence 2000:  According to the confession , on five occasions he got to know women via Internet or telephone contacts , who he then sent out onto the streets against their will after a few weeks .                                               \n",
            "valuation - target sentence 2000:  <unk> hat er in <unk> <unk> Frauen über <unk> oder <unk> <unk> , die er nach einigen Wochen gegen ihren <unk> auf den <unk> <unk> . \n",
            "valuation - target sentence 2100:  People are paying more directly into what they are getting .                                                                     \n",
            "valuation - target sentence 2100:  Die Leute <unk> direkt für das , was sie bekommen . \n",
            "valuation - target sentence 2200:  This year , Americans will spend around $ 106 million on <unk> , according to the U.S. Census Bureau .                                     \n",
            "valuation - target sentence 2200:  Dieses Jahr werden <unk> etwa 106 <unk> Dollar für <unk> <unk> , so das US <unk> <unk> . \n",
            "valuation - target sentence 2300:  He said : &quot; In some cases , it has reached too far inappropriately . &quot;                                     \n",
            "valuation - target sentence 2300:  Er <unk> : „ In einigen <unk> ist das <unk> zu weit <unk> . “ \n",
            "valuation - target sentence 2400:  The devices , which track every mile a <unk> drives and transmit that information to bureaucrats , are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America &apos;s major roads .         \n",
            "valuation - target sentence 2400:  Die <unk> , die jeden <unk> <unk> <unk> und die Informationen an die <unk> <unk> , sind <unk> eines <unk> <unk> von Washington und den <unk> der <unk> , das <unk> System zur <unk> US ##AT##-##AT## <unk> <unk> zu <unk> . \n",
            "valuation - target sentence 2500:  Since 2010 , Johanna <unk> has occupied a Chair for Systematic Theology in the Institute for Catholic Theology at the University of Kassel .                                \n",
            "valuation - target sentence 2500:  Seit 2010 hat <unk> <unk> einen <unk> für <unk> <unk> am Institut für <unk> <unk> der Universität Kassel <unk> . \n",
            "valuation - target sentence 2600:  During the course of the day one thing became particularly clear : the scepticism towards the new system is reluctantly yet visibly giving way to confidence .                           \n",
            "valuation - target sentence 2600:  Im <unk> des <unk> wurde besonders deutlich : die <unk> gegenüber dem <unk> <unk> <unk> aber <unk> einer <unk> . \n",
            "valuation - target sentence 2700:  The syllabus will place a greater focus on &quot; real world problems , &quot; including financial mathematics .                                              \n",
            "valuation - target sentence 2700:  Im <unk> wird mehr <unk> auf „ Probleme aus dem <unk> Leben “ <unk> , <unk> <unk> . \n",
            "Epoch: 4, Train loss: 0.540, Val loss: 21.225, blue score = 0.0065,Epoch time = 4246.356s\n",
            "start epoch: 5\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  It is not yet clear how the new owner will use the former Kindergarten .                                            \n",
            "valuation - target sentence 100:  <unk> nicht <unk> ist , wie der neue <unk> den <unk> Kindergarten nutzen will . \n",
            "valuation - target sentence 200:  He said European leaders need to be honest with their own people about the kind of espionage programs they &apos;ve used for years themselves .                                      \n",
            "valuation - target sentence 200:  Er <unk> , die <unk> <unk> <unk> ihrer <unk> gegenüber <unk> über die Art von <unk> sein , die sie selbst seit Jahren <unk> . \n",
            "valuation - target sentence 300:  Among others , the question was raised as to why <unk> and detention reservoirs are no longer planned to protect the location .                                    \n",
            "valuation - target sentence 300:  Unter anderem wurde in Frage gestellt , <unk> nicht mehr <unk> und <unk> zum Schutz des <unk> <unk> <unk> . \n",
            "valuation - target sentence 400:  He got too close to certain clients , in particular Scarborough , and he allowed his independence to be compromised .                              \n",
            "valuation - target sentence 400:  Er stand <unk> <unk> zu <unk> , insbesondere <unk> , und er <unk> es zu , dass seine <unk> <unk> wurde . \n",
            "valuation - target sentence 500:  Whether this person was living in the house is as yet unclear , said a police spokesperson in Bayreuth .                                          \n",
            "valuation - target sentence 500:  <unk> es sich um einen <unk> <unk> sei noch <unk> , wie ein <unk> der <unk> in <unk> <unk> . \n",
            "valuation - target sentence 600:  The system is fitted with coloured LEDs , which are bright enough that drivers can easily see the lights , even when the sun is low in the sky .                                                  \n",
            "valuation - target sentence 600:  Die Anlage ist mit <unk> LEDs ausgestattet , die so <unk> <unk> , dass die <unk> von den <unk> <unk> auch bei <unk> Sonne gut zu <unk> sind . \n",
            "valuation - target sentence 700:  &quot; However , it will not merely be a chronicle of <unk> itself , but rather a printed work on the <unk> of today , which now consists of <unk> , <unk> ##AT##-##AT## <unk> , <unk> , <unk> and <unk> , &quot; said Mayor Walter Hengstler .                 \n",
            "valuation - target sentence 700:  &quot; Es soll aber eine <unk> nicht nur von <unk> selbst werden , sondern ein <unk> Werk über das <unk> von heute , das <unk> aus <unk> , <unk> ##AT##-##AT## <unk> , <unk> , <unk> und <unk> besteht &quot; , so <unk> Walter <unk> . \n",
            "valuation - target sentence 800:  Salem : Johanna <unk> at the Ecumenical Discussion Forum                                          \n",
            "valuation - target sentence 800:  <unk> : mt <unk> beim <unk> <unk> \n",
            "valuation - target sentence 900:  Someone in the German parliament says we should build a German Google .                                                           \n",
            "valuation - target sentence 900:  Im <unk> wurde <unk> , wir sollten ein <unk> Google <unk> . \n",
            "valuation - target sentence 1000:  This is almost ten times as many as in Germany , although with a population of 82 million people , Germany has a much higher number of residents than South Africa ( 50 million ) .                                                        \n",
            "valuation - target sentence 1000:  Das sind fast <unk> so viel wie in Deutschland , das aber mit 82 <unk> Menschen deutlich mehr <unk> hat als <unk> ( 50 <unk> ) . \n",
            "valuation - target sentence 1100:  Some bankers have tried to play down the affair by saying the vast and highly liquid foreign exchange market is almost impossible to manipulate , but senior traders are saying this is not necessarily true .                        \n",
            "valuation - target sentence 1100:  <unk> <unk> haben <unk> , die <unk> <unk> und <unk> , dass es <unk> <unk> sei , den <unk> und <unk> <unk> <unk> zu <unk> , doch <unk> <unk> sind der <unk> , dass dies nicht <unk> <unk> sei . \n",
            "valuation - target sentence 1200:  Since Scotland isn &apos;t in the Schengen area now , continued non ##AT##-##AT## compliance would be a cheap concession for Brussels to offer up in return for whatever it really wanted out of the Scots .           \n",
            "valuation - target sentence 1200:  Da <unk> <unk> nicht zum <unk> ##AT##-##AT## Raum <unk> , wäre die <unk> <unk> für <unk> nur ein <unk> <unk> im <unk> für das , was man wirklich von den <unk> will . \n",
            "valuation - target sentence 1300:  <unk> was given <unk> as a third birthday present last March and has practised with the Shetland pony every day since .                           \n",
            "valuation - target sentence 1300:  <unk> hatte <unk> letzten März als <unk> zu seinem <unk> <unk> bekommen und <unk> jeden Tag mit dem <unk> ##AT##-##AT## <unk> <unk> . \n",
            "valuation - target sentence 1400:  George <unk> : Europe break ##AT##-##AT## up gives Scots choice                                                          \n",
            "valuation - target sentence 1400:  George <unk> : <unk> <unk> gibt den <unk> die Wahl \n",
            "valuation - target sentence 1500:  Mr Palmer , 59 , said his policies included an international airport for the Sunshine Coast and he would take &quot; very seriously &quot; his new job .                     \n",
            "valuation - target sentence 1500:  <unk> , 59 , <unk> , dass zu seiner Politik ein <unk> Flughafen für die <unk> Coast <unk> und dass er sein neues <unk> „ sehr <unk> “ nehmen werde . \n",
            "valuation - target sentence 1600:  Suitable clothing is desirable - for example competitors can wear <unk> .                                 \n",
            "valuation - target sentence 1600:  <unk> ist eine <unk> <unk> , <unk> können die <unk> im <unk> <unk> . \n",
            "valuation - target sentence 1700:  The Israeli government had previously repeatedly warned that any attempt by Syria to supply Hezbollah with chemical or other dangerous weapons would be crossing a &quot; red line &quot; , which would result in a military response .             \n",
            "valuation - target sentence 1700:  Die <unk> <unk> hatte <unk> <unk> <unk> , dass jeder <unk> <unk> , die <unk> mit <unk> oder anderen <unk> Waffen zu <unk> , das <unk> einer &quot; <unk> <unk> &quot; <unk> , dem eine <unk> <unk> <unk> würde . \n",
            "valuation - target sentence 1800:  <unk> and Brooks deny conspiring with others to hack phones between October 3 2000 and August 9 2006 .                             \n",
            "valuation - target sentence 1800:  <unk> und <unk> <unk> ab , sich mit anderen <unk> zu haben , zwischen dem 3. Oktober 2000 und dem <unk> August 2006 <unk> zu <unk> . \n",
            "valuation - target sentence 1900:  &quot; Depending on the condition of the plan , we could approve expanded use of electronic devices very soon , &quot; the FAA said in a statement .                                    \n",
            "valuation - target sentence 1900:  „ Je nach <unk> des <unk> <unk> wir die <unk> Nutzung <unk> <unk> sehr bald <unk> “ , gab die <unk> in einer <unk> bekannt . \n",
            "valuation - target sentence 2000:  Business leaders are sceptical about this .                                             \n",
            "valuation - target sentence 2000:  Die Unternehmen sind <unk> <unk> . \n",
            "valuation - target sentence 2100:  Her house is located in a peaceful neighbourhood in Stuttgart , with plenty of greenery , an old tree population , nice neighbours and not too much traffic .                       \n",
            "valuation - target sentence 2100:  Ihr Haus liegt in einer <unk> <unk> von Stuttgart mit viel <unk> , einem alten <unk> , <unk> <unk> und auch nicht <unk> viel <unk> . \n",
            "valuation - target sentence 2200:  Why is it that we shouldn &apos;t have a system where you can walk in , punch your details into a computer , vote immediately and have a result at 6.30 that night ?                         \n",
            "valuation - target sentence 2200:  <unk> sollten wir kein System haben , bei dem man <unk> , die Daten in einen Computer <unk> , sofort <unk> und dann um 6 : 30 Uhr <unk> das <unk> hat ? \n",
            "valuation - target sentence 2300:  But concerns have grown after Mr <unk> was quoted as saying <unk> was abandoning the 1992 peace accord .                                        \n",
            "valuation - target sentence 2300:  <unk> die <unk> haben <unk> , seit <unk> mit den <unk> <unk> wurde , <unk> <unk> das <unk> von 1992 auf . \n",
            "valuation - target sentence 2400:  Rescue workers entered the premises and found the body in a bedroom .                                       \n",
            "valuation - target sentence 2400:  Die <unk> <unk> in die <unk> und <unk> die <unk> in einem Zimmer . \n",
            "valuation - target sentence 2500:  After the first three matches and a lunch break with pasta , the tournament moved into the crucial phase .                                     \n",
            "valuation - target sentence 2500:  Nach den ersten drei <unk> und einer <unk> mit Pasta ging das <unk> in die <unk> Phase . \n",
            "valuation - target sentence 2600:  It also says research shows cabin experience depends on more than the width of a seat .                                      \n",
            "valuation - target sentence 2600:  Das Unternehmen <unk> auch , laut <unk> <unk> das <unk> im <unk> von mehr als der <unk> ab . \n",
            "valuation - target sentence 2700:  The judge had ruled in August the city violated the Constitution in the way it carried out its program of stopping and questioning people .                     \n",
            "valuation - target sentence 2700:  Die <unk> hatte im August <unk> , die Stadt <unk> gegen die <unk> mit der Art und Weise , wie das Programm für das <unk> und <unk> von Personen <unk> werde . \n",
            "Epoch: 5, Train loss: 0.304, Val loss: 21.891, blue score = 0.0071,Epoch time = 4251.962s\n",
            "start epoch: 6\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  In May , in the meeting of the Town Council , he said that he feared for the outcome of the spa should the population vote against the plans for a spa and wellness complex on the Schweizer <unk> in the local referendum .            \n",
            "valuation - target sentence 100:  Mai hatte in der <unk> <unk> , dass er das Aus für das <unk> <unk> , sollte die <unk> beim <unk> gegen die <unk> für einen <unk> und <unk> auf der Schweizer <unk> <unk> . \n",
            "valuation - target sentence 200:  None of these traders have been accused of any wrongdoing .                                         \n",
            "valuation - target sentence 200:  <unk> dieser <unk> wurde <unk> <unk> <unk> . \n",
            "valuation - target sentence 300:  During his arrest <unk> picked up his wallet and tried to remove several credit cards but they were all seized and a hair sample was taken <unk> him .                \n",
            "valuation - target sentence 300:  Während seiner <unk> <unk> <unk> seine <unk> und <unk> , eine <unk> von <unk> zu <unk> , doch sie wurden <unk> und ihm wurde eine <unk> <unk> . \n",
            "valuation - target sentence 400:  The 2nd U.S. Circuit Court of Appeals said the decisions of Judge <unk> <unk> will be stayed pending the outcome of an appeal by the city .                      \n",
            "valuation - target sentence 400:  Das 2. US ##AT##-##AT## <unk> <unk> , die <unk> von <unk> <unk> <unk> <unk> <unk> , bis über eine <unk> der Stadt <unk> sei . \n",
            "valuation - target sentence 500:  This year , there were even more absolute top athletes at the starting line for the main race .                                        \n",
            "valuation - target sentence 500:  Im <unk> waren in diesem Jahr noch mehr absolute <unk> am Start . \n",
            "valuation - target sentence 600:  Anyone who waits for less than five minutes has , under certain circumstances , not waited long enough , warned Becker , referring to a ruling of the Hamm Higher Regional Court .                   \n",
            "valuation - target sentence 600:  Wer sich weniger als <unk> Minuten <unk> , <unk> unter <unk> nicht lange <unk> , <unk> <unk> und <unk> auf einen <unk> des <unk> <unk> . \n",
            "valuation - target sentence 700:  In fact , there are 1,200 officially sanctioned haunted houses in the United States generating about $ 500 million in revenue , according to America <unk> , and that includes those awesome photos of you mid ##AT##-##AT## peeing your pants that your friend puts on Facebook and you can &apos;t take down and then that guy you like sees the photo and leaves a comment like &quot; nice face . &quot; \n",
            "valuation - target sentence 700:  Es gibt <unk> 1.200 <unk> <unk> <unk> in den <unk> <unk> , die 500 <unk> Dollar <unk> <unk> , so America <unk> , und das <unk> die <unk> Fotos ein , auf denen man sich gerade in die Hose <unk> und die Freunde dann auf Facebook stellen , wo man sie nicht wieder <unk> , und einem dann der <unk> , auf den man steht , einen <unk> <unk> wie „ <unk> <unk> “ . \n",
            "valuation - target sentence 800:  The Treasury have provisionally <unk> out the <unk> measure but are awaiting a final decision from Mr Osborne , who , in the 2012 Budget , introduced a 7 % rate of stamp duty for homes costing more than £ 2m and annual charges for buyers who choose to hold homes in a company rather than as individuals . \n",
            "valuation - target sentence 800:  Das <unk> hat <unk> die <unk> ##AT##-##AT## <unk> <unk> , <unk> aber noch auf die <unk> <unk> von <unk> George <unk> , der im <unk> 2012 eine <unk> in Höhe von 7 % auf <unk> <unk> hatte , die über zwei <unk> <unk> <unk> sowie <unk> <unk> für Käufer , die <unk> als Firma und nicht als <unk> <unk> . \n",
            "valuation - target sentence 900:  Half a mashed , ripe banana , for example , can replace an egg as a binding agent in a cake .                                     \n",
            "valuation - target sentence 900:  Eine <unk> <unk> , <unk> <unk> zum Beispiel kann ein <unk> als <unk> in einem <unk> <unk> . \n",
            "valuation - target sentence 1000:  &quot; I find it wonderful that in America , myth and folklore already has a part in education , &quot; he said .                            \n",
            "valuation - target sentence 1000:  „ Ich <unk> es <unk> , dass in <unk> <unk> und Folklore bereits Teil der Bildung sind “ , <unk> er . \n",
            "valuation - target sentence 1100:  In Nevada , where about 50 volunteers &apos; cars were equipped with the devices not long ago , drivers were uneasy about the government being able to monitor their every move .                 \n",
            "valuation - target sentence 1100:  In Nevada , wo vor <unk> 50 <unk> mit den <unk> ausgestattet wurden , waren <unk> <unk> beim <unk> , die <unk> könnte jede ihrer <unk> <unk> . \n",
            "valuation - target sentence 1200:  The $ 325 million package includes a $ 31 million elective surgery <unk> .                                             \n",
            "valuation - target sentence 1200:  Das <unk> ##AT##-##AT## <unk> ##AT##-##AT## Dollar ##AT##-##AT## <unk> <unk> 31 <unk> Dollar <unk> für <unk> . \n",
            "valuation - target sentence 1300:  He is offering every citizen the opportunity to focus on nature itself .                                                                     \n",
            "valuation - target sentence 1300:  Er <unk> <unk> Bürger die Möglichkeit , in der Natur selbst <unk> zu <unk> . \n",
            "valuation - target sentence 1400:  Even the most recent statements from <unk> will not change this fact .                                              \n",
            "valuation - target sentence 1400:  Auch die <unk> <unk> von <unk> <unk> daran nichts <unk> . \n",
            "valuation - target sentence 1500:  News reports that the NSA swept up millions of phone records in Europe have <unk> relations with some U.S. allies , though the agency &apos;s chief said this week that they were inaccurate and reflected a misunderstanding of metadata that Nato allies collected and shared with the United States .   \n",
            "valuation - target sentence 1500:  <unk> , <unk> die <unk> <unk> von <unk> in Europa <unk> hat , haben die <unk> zu einigen US ##AT##-##AT## <unk> <unk> , <unk> der <unk> des <unk> diese Woche <unk> , die <unk> <unk> <unk> und <unk> auf einem <unk> der <unk> , die von <unk> ##AT##-##AT## <unk> <unk> und mit den <unk> <unk> <unk> worden <unk> . \n",
            "valuation - target sentence 1600:  The police inspector responsible at the time was invited as a witness .                                                  \n",
            "valuation - target sentence 1600:  Als <unk> war der <unk> <unk> <unk> <unk> . \n",
            "valuation - target sentence 1700:  Climate change is taken into account in the form of an allowance of 15 per cent .                                  \n",
            "valuation - target sentence 1700:  Der <unk> wird durch einen 15 ##AT##-##AT## <unk> <unk> Rechnung <unk> . \n",
            "valuation - target sentence 1800:  &quot; On both days , adults and children alike can learn various traditional handicrafts , &quot; said the press release .                                \n",
            "valuation - target sentence 1800:  &quot; <unk> und Klein können an beiden Tagen verschiedene <unk> <unk> <unk> &quot; , <unk> es in der <unk> . \n",
            "valuation - target sentence 1900:  According to information from the Regional Church , which covers three quarters of Lower Saxony , around 60 pastors retire every year .                       \n",
            "valuation - target sentence 1900:  Nach Angaben der <unk> , die drei <unk> <unk> <unk> , gehen <unk> pro Jahr rund 60 <unk> in den <unk> . \n",
            "valuation - target sentence 2000:  Plant whipping cream can replace traditional whipping cream .                                               \n",
            "valuation - target sentence 2000:  <unk> <unk> <unk> <unk> <unk> . \n",
            "valuation - target sentence 2100:  You almost think that you can see the folds in the old man &apos;s neck , of which young fisherman <unk> speaks .                                         \n",
            "valuation - target sentence 2100:  Man <unk> fast , die <unk> im <unk> des alten <unk> , von denen der <unk> <unk> <unk> , sehen zu können . \n",
            "valuation - target sentence 2200:  They &apos;re not all experienced racers , but people looking for excitement and adventure , and an achievable path towards world ##AT##-##AT## class events .                                       \n",
            "valuation - target sentence 2200:  Sie sind nicht alle <unk> <unk> , sondern Leute , die auf der Suche nach <unk> und <unk> sind sowie nach einem <unk> Weg zu <unk> ##AT##-##AT## <unk> . \n",
            "valuation - target sentence 2300:  I &apos;ve realised my wrongdoing .                                     \n",
            "valuation - target sentence 2300:  Mir ist <unk> <unk> , dass ich <unk> <unk> habe . \n",
            "valuation - target sentence 2400:  Passengers who buy their tickets on the airline &apos;s website won &apos;t have to pay .                                  \n",
            "valuation - target sentence 2400:  <unk> , die Ihre Tickets auf der Website der <unk> <unk> , müssen nicht <unk> . \n",
            "valuation - target sentence 2500:  <unk> billion years ago , our cosmos was created by the Big Bang .                                 \n",
            "valuation - target sentence 2500:  Vor <unk> <unk> Jahren <unk> unser <unk> durch den <unk> . \n",
            "valuation - target sentence 2600:  Supreme Court upholds Obama health care law                                                  \n",
            "valuation - target sentence 2600:  Label Court <unk> <unk> <unk> \n",
            "valuation - target sentence 2700:  To get 20 years , after killing 202 people and injuring many hundreds , it &apos;s not much .                                                      \n",
            "valuation - target sentence 2700:  20 Jahre zu bekommen , <unk> er <unk> Menschen <unk> und viele <unk> <unk> hat , ist nicht viel . \n",
            "Epoch: 6, Train loss: 0.284, Val loss: 22.752, blue score = 0.0075,Epoch time = 4250.432s\n",
            "start epoch: 7\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  During the proceedings the public prosecution also emphasised that they were convinced that <unk> was alone at the time of the attack .                            \n",
            "valuation - target sentence 100:  Auch die <unk> <unk> während des <unk> , sie sei <unk> , dass <unk> zum <unk> der <unk> <unk> war . \n",
            "valuation - target sentence 200:  The first test plane was unveiled in March and took flight for the first time in September after months of delays .                     \n",
            "valuation - target sentence 200:  Das erste <unk> wurde im März der <unk> <unk> und <unk> nach <unk> <unk> <unk> im September . \n",
            "valuation - target sentence 300:  The Administrative Department had Deutsche Bahn &apos;s reconstruction plans investigated by the <unk> Engineering Office , as they had doubts as to whether the planned connection of the <unk> Weg will be straightforward .                              \n",
            "valuation - target sentence 300:  Die <unk> hatte die <unk> der Bahn vom <unk> <unk> <unk> lassen , da sie <unk> daran hatte , ob der <unk> <unk> des <unk> <unk> dann noch ohne <unk> möglich sei . \n",
            "valuation - target sentence 400:  During the trial , <unk> admitted to having been in a bar with <unk> on the night of the attack .                                   \n",
            "valuation - target sentence 400:  <unk> hatte während des <unk> <unk> , in der <unk> mit <unk> in einer Bar <unk> zu sein . \n",
            "valuation - target sentence 500:  Except : In the <unk> of those in the passenger and back seats , hunger strikes .                                                               \n",
            "valuation - target sentence 500:  Nur : In den <unk> der Leute auf dem <unk> und <unk> macht sich der Hunger <unk> . \n",
            "valuation - target sentence 600:  Secret Service <unk> , Edward <unk> , has a fundamental interest in helping Germany clarify the increasingly explosive NSA espionage scandal .                                         \n",
            "valuation - target sentence 600:  Der <unk> ##AT##-##AT## <unk> Edward <unk> hat <unk> Interesse , Deutschland bei der <unk> der immer <unk> <unk> <unk> ##AT##-##AT## <unk> zu <unk> . \n",
            "valuation - target sentence 700:  According to Arnold , every possible test was carried out prior to the selection of the location for the traffic light posts : &quot; Using a goods vehicle loaded with particularly long tree trunks , we also tested whether such vehicles could access the B 33 from the <unk> without knocking over the traffic light posts &quot; . \n",
            "valuation - target sentence 700:  Es wurde laut Arnold bei der <unk> der <unk> <unk> alles <unk> : &quot; <unk> eines extra für uns mit besonders <unk> <unk> <unk> <unk> haben wir <unk> , ob diese <unk> aus dem <unk> auf die B 33 <unk> können , ohne den <unk> <unk> &quot; . \n",
            "valuation - target sentence 800:  I credit Obama with great and varied accomplishments , from the passage of the Affordable Care Act to our military exit from Iraq , the end of &quot; don &apos;t ask don &apos;t tell , &quot; to the killing of Osama bin Laden .                            \n",
            "valuation - target sentence 800:  Ich <unk> die <unk> und <unk> <unk> an , die Obama <unk> hat , von der <unk> des <unk> Care Act für <unk> bis zum <unk> <unk> aus dem <unk> , dem Ende des <unk> <unk> von <unk> in der <unk> und der <unk> <unk> bin <unk> . \n",
            "valuation - target sentence 900:  The investigation came to the conclusion that it could not be ruled out that the inclination of the road could result in longer vehicles becoming stuck .                         \n",
            "valuation - target sentence 900:  Die <unk> kam zu dem <unk> , dass nicht <unk> sei , dass die <unk> der Straße dazu <unk> <unk> , dass <unk> <unk> <unk> . \n",
            "valuation - target sentence 1000:  I have been advocating it as a plan for twenty years .                                        \n",
            "valuation - target sentence 1000:  Ich habe das schon seit <unk> Jahren als Plan <unk> . \n",
            "valuation - target sentence 1100:  There aren &apos;t many doctors in the west African country ; just one for every 5,000 people                                        \n",
            "valuation - target sentence 1100:  In dem <unk> Land gibt es nicht viele <unk> , nur einen für 5.000 Menschen \n",
            "valuation - target sentence 1200:  He was telling me that , when he &apos;s here , sometimes he &apos;ll walk down an alley between two buildings and think to himself , Hey , if I lose all my money , maybe I &apos;ll live here .             \n",
            "valuation - target sentence 1200:  Er hat mir <unk> , wenn er hier ist , dann <unk> er <unk> durch die <unk> zwischen zwei <unk> und <unk> sich : <unk> , wenn ich mein <unk> Geld <unk> , dann <unk> ich vielleicht hier . \n",
            "valuation - target sentence 1300:  In the clubhouse at the &quot; <unk> &quot; Inn in <unk> , all those present enjoyed the friendly get ##AT##-##AT## together in the name of music .                   \n",
            "valuation - target sentence 1300:  Im <unk> der <unk> &quot; <unk> &quot; in <unk> <unk> alle das <unk> <unk> ganz im <unk> der Musik . \n",
            "valuation - target sentence 1400:  The conflict has forced some 2 million people to flee the country .                                    \n",
            "valuation - target sentence 1400:  Der <unk> hat etwa zwei <unk> Menschen zur <unk> aus dem Land <unk> . \n",
            "valuation - target sentence 1500:  30 ideas were up for debate , of which five have now made it onto the short list .                                                 \n",
            "valuation - target sentence 1500:  30 <unk> <unk> zur <unk> , <unk> davon sind nun in die <unk> Wahl <unk> . \n",
            "valuation - target sentence 1600:  &quot; Money presented the case as a successful case of transition , but it was not , &quot; said <unk> .                                \n",
            "valuation - target sentence 1600:  „ Money <unk> diesen Fall als <unk> Beispiel für eine <unk> , was er aber nicht war “ , so <unk> . \n",
            "valuation - target sentence 1700:  &quot; In aerospace , results were in line with our guidance , but the low order intake and overall market conditions were a disappointment , &quot; <unk> said .                               \n",
            "valuation - target sentence 1700:  „ Im Bereich <unk> und <unk> <unk> die Ergebnisse im Rahmen unserer <unk> , aber der <unk> <unk> und die <unk> insgesamt waren <unk> “ , <unk> <unk> . \n",
            "valuation - target sentence 1800:  The goal of this adjustment is to create a more relaxing atmosphere .                                \n",
            "valuation - target sentence 1800:  Eine <unk> Atmosphäre zu <unk> , ist das Ziel dieser <unk> . \n",
            "valuation - target sentence 1900:  The potential was there that she could have been very seriously injured or worse .                                       \n",
            "valuation - target sentence 1900:  Es <unk> die Möglichkeit , dass sie sehr <unk> <unk> war oder <unk> . \n",
            "valuation - target sentence 2000:  On Saturday 26 October , this exceptional mass will be heard from 7.00 p.m. during the evening mass in the St. <unk> <unk> and on Sunday 27 October , from <unk> , at the service in the St. Johannes <unk> in <unk> .                                                 \n",
            "valuation - target sentence 2000:  Am Samstag , <unk> Oktober , wird diese <unk> Messe ab 19 während der <unk> in der St. <unk> <unk> und am Sonntag , <unk> Oktober , ab <unk> Uhr beim <unk> in der St. Johannes <unk> in <unk> zu <unk> sein . \n",
            "valuation - target sentence 2100:  There are lots of well ##AT##-##AT## publicized theories about the causes of <unk> puberty .                            \n",
            "valuation - target sentence 2100:  Es gibt viele <unk> <unk> <unk> über die <unk> <unk> <unk> . \n",
            "valuation - target sentence 2200:  Thomas <unk> , the MP who heads the parliamentary panel that oversees intelligence , said that if there were an opportunity to hear Mr <unk> as a witness &quot; without bringing him into danger and completely ruining relations with the US , &quot; it should be taken . \n",
            "valuation - target sentence 2200:  Thomas <unk> , der <unk> , der den für den <unk> <unk> <unk> <unk> <unk> , <unk> , man <unk> die <unk> <unk> , <unk> als <unk> <unk> , wenn dies möglich sei , „ ohne ihn zu <unk> und die <unk> zu den USA <unk> zu <unk> “ . \n",
            "valuation - target sentence 2300:  In a statement on their official website , the Arctic <unk> said : &quot; Following the decision to postpone the show at the Birmingham LG Arena tonight and after seeking medical advice , Arctic <unk> must also postpone the show at the Glasgow Hydro on Friday , November 1 . &quot; \n",
            "valuation - target sentence 2300:  In einer <unk> auf ihrer <unk> Website <unk> die <unk> <unk> mit : „ Nach der <unk> , die Show in der <unk> Arena in Birmingham heute <unk> <unk> sowie auf <unk> Rat müssen die <unk> <unk> auch die Show im <unk> in Glasgow am Freitag , dem 1. November <unk> . “ \n",
            "valuation - target sentence 2400:  We will find you and put you before the courts .                                              \n",
            "valuation - target sentence 2400:  Wir finden Sie und bringen Sie vor <unk> . \n",
            "valuation - target sentence 2500:  In addition to holding these weekend clinics and working as a hospital surgeon , <unk> also works nights at private medical clinics around <unk> .                                           \n",
            "valuation - target sentence 2500:  Neben den <unk> und seiner Arbeit als <unk> im <unk> <unk> <unk> auch <unk> in <unk> <unk> in <unk> . \n",
            "valuation - target sentence 2600:  The introduction of a toll for cars would also only be a first step , in <unk> &apos;s eyes .                               \n",
            "valuation - target sentence 2600:  Die <unk> einer <unk> für alle <unk> wäre aus <unk> <unk> <unk> nur ein <unk> <unk> . \n",
            "valuation - target sentence 2700:  People are paying more directly into what they are getting .                                                                       \n",
            "valuation - target sentence 2700:  Die Leute <unk> direkt für das , was sie bekommen . \n",
            "Epoch: 7, Train loss: 0.268, Val loss: 22.751, blue score = 0.0073,Epoch time = 4248.125s\n",
            "start epoch: 8\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  The precise processes for the formation of stars from large gas clouds - and thus the formation of the planets that orbit these stars - lie in proverbial darkness .                  \n",
            "valuation - target sentence 100:  Die <unk> <unk> bei der <unk> von <unk> aus großen <unk> - und damit auch der <unk> von <unk> , die diese Sterne <unk> - liegen immer noch im <unk> <unk> . \n",
            "valuation - target sentence 200:  Court blocks ruling on <unk> stop ##AT##-##AT## and ##AT##-##AT## <unk> policy                                                \n",
            "valuation - target sentence 200:  <unk> <unk> <unk> zur Stop ##AT##-##AT## and ##AT##-##AT## <unk> ##AT##-##AT## <unk> des <unk> \n",
            "valuation - target sentence 300:  &quot; If you want you can take a peaceful stroll along a section of the so ##AT##-##AT## called <unk> <unk> , &quot; says Schulze , encouraging the guests .                                  \n",
            "valuation - target sentence 300:  &quot; Sie können ruhig ein <unk> <unk> der <unk> <unk> <unk> <unk> , wenn Sie wollen &quot; , <unk> <unk> die Gäste auf . \n",
            "valuation - target sentence 400:  The organisation of the tournament was placed in the <unk> hands of Anja <unk> , who was supported by Carmen Müller and <unk> <unk> .                           \n",
            "valuation - target sentence 400:  Die <unk> lag in den <unk> <unk> von <unk> <unk> , <unk> wurde sie von Carmen Müller und <unk> <unk> . \n",
            "valuation - target sentence 500:  The announcement by the Organization for the Prohibition of Chemical Weapons came one day ahead of the <unk>                                      \n",
            "valuation - target sentence 500:  Die <unk> der Organisation für das <unk> <unk> Waffen ( <unk> ) kam einen Tag vor dem <unk> am \n",
            "valuation - target sentence 600:  Meanwhile , former chief reporter Neville <unk> and former reporter James <unk> oversaw surveillance of Ms <unk> &apos;s movements .                                                              \n",
            "valuation - target sentence 600:  <unk> <unk> der <unk> <unk> <unk> <unk> und der <unk> <unk> James <unk> die <unk> der <unk> von <unk> . \n",
            "valuation - target sentence 700:  The potential switch was a closely guarded secret within the Chicago campaign infrastructure and inside the <unk> Office .                                     \n",
            "valuation - target sentence 700:  Der <unk> <unk> war ein <unk> <unk> <unk> innerhalb der <unk> in Chicago und im <unk> Office . \n",
            "valuation - target sentence 800:  The <unk> <unk> black in the storage yard in front of the embankment .                                           \n",
            "valuation - target sentence 800:  Schwarz <unk> die <unk> auf dem <unk> vor dem <unk> . \n",
            "valuation - target sentence 900:  &quot; Back in the middle of the 20th century , it was called a &apos; psychiatric emergency , &apos; &quot; said <unk> .                            \n",
            "valuation - target sentence 900:  „ Mitte des <unk> <unk> <unk> man das einen , <unk> <unk> ‘ “ , <unk> <unk> . \n",
            "valuation - target sentence 1000:  When she rebels , she experiences extreme cruelty at the hands of a <unk> sadistic Mother Superior and becomes an object of erotic fascination for another .                     \n",
            "valuation - target sentence 1000:  Als sie <unk> , ist sie mit der <unk> <unk> einer <unk> <unk> <unk> <unk> und wird zum <unk> <unk> <unk> einer anderen . \n",
            "valuation - target sentence 1100:  He then became the first high ##AT##-##AT## ranking member of the U.S government to admit that US spying had crossed the line , but emphasised that no one &apos;s rights had been abused .                       \n",
            "valuation - target sentence 1100:  <unk> <unk> er als <unk> <unk> <unk> der US ##AT##-##AT## <unk> <unk> ein , dass die US ##AT##-##AT## <unk> eine <unk> <unk> habe , <unk> aber auch , dass keine <unk> <unk> worden <unk> . \n",
            "valuation - target sentence 1200:  A combined English literature and language course will be scrapped .                                                \n",
            "valuation - target sentence 1200:  Der <unk> <unk> aus <unk> <unk> und Sprache wird <unk> . \n",
            "valuation - target sentence 1300:  Germany became the first European nation to recognize a third gender for babies born with ambiguous <unk> .                               \n",
            "valuation - target sentence 1300:  Deutschland ist die erste <unk> Nation , in der ein <unk> <unk> für Kinder <unk> ist , die mit nicht <unk> <unk> <unk> wurden . \n",
            "valuation - target sentence 1400:  Secret Service <unk> , Edward <unk> , has a fundamental interest in helping Germany clarify the increasingly explosive NSA espionage scandal .                                        \n",
            "valuation - target sentence 1400:  Der <unk> ##AT##-##AT## <unk> Edward <unk> hat <unk> Interesse , Deutschland bei der <unk> der immer <unk> <unk> <unk> ##AT##-##AT## <unk> zu <unk> . \n",
            "valuation - target sentence 1500:  Germany &apos;s largest national church has launched a campaign advertising the minister &apos;s position .                                    \n",
            "valuation - target sentence 1500:  Deutschlands <unk> <unk> hat eine <unk> , um für den <unk> des <unk> zu <unk> . \n",
            "valuation - target sentence 1600:  &quot; Bottom line is that with costs rising , people in the middle to lower end &#91; of the income scale &#93; will be looking to supplement their income wherever they can , &quot; says Song <unk> <unk> , economist at <unk> , a Malaysian bank .                     \n",
            "valuation - target sentence 1600:  „ Im <unk> <unk> das , dass <unk> <unk> Kosten die Menschen im <unk> bis <unk> Segment ( der <unk> ) <unk> werden , ihr <unk> zu <unk> , wo immer das möglich ist “ , <unk> Song <unk> <unk> , <unk> bei <unk> , einer <unk> Bank . \n",
            "valuation - target sentence 1700:  The best moments of the evening is when the singing starts - tracks range from Deep Purple to traditional folk songs .                                                                      \n",
            "valuation - target sentence 1700:  Die besten <unk> hat der <unk> , wenn <unk> wird - die <unk> <unk> von Deep <unk> bis zu <unk> <unk> . \n",
            "valuation - target sentence 1800:  Virgin , which has already been talking to CASA about extending the use its in ##AT##-##AT## flight wi ##AT##-##AT## fi entertainment system , was amenable to a change but said it would take its lead from the regulator .                                 \n",
            "valuation - target sentence 1800:  <unk> hat bereits mit der <unk> <unk> über eine <unk> <unk> Wi ##AT##-##AT## Fi ##AT##-##AT## <unk> auf <unk> <unk> und <unk> , man sei <unk> für <unk> , diese <unk> aber vom <unk> <unk> werden . \n",
            "valuation - target sentence 1900:  The verdict comes ahead of the 10th anniversary of the attack later this year , which will be marked by ceremonies in Bali and Australia .                               \n",
            "valuation - target sentence 1900:  Das <unk> <unk> vor dem <unk> <unk> des <unk> in diesem Jahr , der mit <unk> in Bali und <unk> <unk> wird . \n",
            "valuation - target sentence 2000:  And then I close the book .                                                                  \n",
            "valuation - target sentence 2000:  Und dann <unk> ich das Buch . \n",
            "valuation - target sentence 2100:  Parents of Georgia teen who died in &apos; freak accident &apos; believe son was murdered                             \n",
            "valuation - target sentence 2100:  <unk> eines <unk> aus Georgia , der bei <unk> <unk> <unk> , <unk> an <unk> \n",
            "valuation - target sentence 2200:  Two out of five people in the country live below the poverty line , and nearly three ##AT##-##AT## quarters of the country &apos;s health ##AT##-##AT## care spending is private .                             \n",
            "valuation - target sentence 2200:  Zwei von <unk> Menschen im Land <unk> <unk> der <unk> und <unk> <unk> der <unk> im Land sind privat . \n",
            "valuation - target sentence 2300:  However , CCTV footage later revealed she had returned to her hotel safely after becoming separated from her Australian boyfriend .                     \n",
            "valuation - target sentence 2300:  <unk> allerdings <unk> die <unk> einer <unk> , dass sie sicher in ihr Hotel <unk> war , <unk> sie von ihrem <unk> <unk> <unk> worden war . \n",
            "valuation - target sentence 2400:  Today at Liverpool Crown Court <unk> , who works at law firm Forbes <unk> , based in <unk> , was facing disgrace after being found guilty of two counts of <unk> the course of justice following a three week trial at Liverpool Crown Court .         \n",
            "valuation - target sentence 2400:  Heute wurde <unk> , der für die <unk> <unk> <unk> in <unk> <unk> , am <unk> Crown Court nach einem <unk> Verfahren in zwei <unk> wegen <unk> der <unk> für <unk> <unk> und <unk> . \n",
            "valuation - target sentence 2500:  Prime property - the top 5 % to 10 % of the housing market by price - in the affluent south ##AT##-##AT## west London belt , which stretches from <unk> to Wimbledon , has increased by a record 11.8 % over the past year .              \n",
            "valuation - target sentence 2500:  <unk> Immobilien – die <unk> <unk> bis <unk> Prozent des <unk> nach Preis – im <unk> <unk> <unk> , der sich von <unk> bis <unk> <unk> , haben sich um den <unk> von <unk> % im letzten Jahr <unk> . \n",
            "valuation - target sentence 2600:  Five matches were scheduled for each team , with every team playing one match against all the others .                                        \n",
            "valuation - target sentence 2600:  Es <unk> <unk> Spiele pro Team auf dem Plan , <unk> jeder gegen jeden <unk> . \n",
            "valuation - target sentence 2700:  <unk> is seeking increased damages against Google , as it claims Google &apos;s patent infringement is willful , according to the complaint .                              \n",
            "valuation - target sentence 2700:  <unk> <unk> einen <unk> <unk> von Google , da es <unk> , <unk> <unk> sei <unk> , so die <unk> . \n",
            "Epoch: 8, Train loss: 0.269, Val loss: 23.220, blue score = 0.0079,Epoch time = 4248.825s\n",
            "start epoch: 9\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n",
            "train batch: 1400\n",
            "train batch: 1500\n",
            "train batch: 1600\n",
            "train batch: 1700\n",
            "train batch: 1800\n",
            "train batch: 1900\n",
            "train batch: 2000\n",
            "train batch: 2100\n",
            "train batch: 2200\n",
            "train batch: 2300\n",
            "train batch: 2400\n",
            "train batch: 2500\n",
            "train batch: 2600\n",
            "train batch: 2700\n",
            "train batch: 2800\n",
            "train batch: 2900\n",
            "train batch: 3000\n",
            "train batch: 3100\n",
            "train batch: 3200\n",
            "train batch: 3300\n",
            "train batch: 3400\n",
            "train batch: 3500\n",
            "train batch: 3600\n",
            "train batch: 3700\n",
            "train batch: 3800\n",
            "train batch: 3900\n",
            "train batch: 4000\n",
            "train batch: 4100\n",
            "train batch: 4200\n",
            "train batch: 4300\n",
            "train batch: 4400\n",
            "train batch: 4500\n",
            "train batch: 4600\n",
            "train batch: 4700\n",
            "train batch: 4800\n",
            "train batch: 4900\n",
            "train batch: 5000\n",
            "train batch: 5100\n",
            "train batch: 5200\n",
            "train batch: 5300\n",
            "train batch: 5400\n",
            "train batch: 5500\n",
            "train batch: 5600\n",
            "train batch: 5700\n",
            "train batch: 5800\n",
            "train batch: 5900\n",
            "train batch: 6000\n",
            "train batch: 6100\n",
            "train batch: 6200\n",
            "train batch: 6300\n",
            "train batch: 6400\n",
            "train batch: 6500\n",
            "train batch: 6600\n",
            "train batch: 6700\n",
            "train batch: 6800\n",
            "train batch: 6900\n",
            "train batch: 7000\n",
            "train batch: 7100\n",
            "train batch: 7200\n",
            "train batch: 7300\n",
            "train batch: 7400\n",
            "train batch: 7500\n",
            "train batch: 7600\n",
            "train batch: 7700\n",
            "train batch: 7800\n",
            "train batch: 7900\n",
            "train batch: 8000\n",
            "train batch: 8100\n",
            "train batch: 8200\n",
            "train batch: 8300\n",
            "train batch: 8400\n",
            "train batch: 8500\n",
            "train batch: 8600\n",
            "train batch: 8700\n",
            "train batch: 8800\n",
            "train batch: 8900\n",
            "train batch: 9000\n",
            "train batch: 9100\n",
            "train batch: 9200\n",
            "train batch: 9300\n",
            "train batch: 9400\n",
            "train batch: 9500\n",
            "train batch: 9600\n",
            "train batch: 9700\n",
            "train batch: 9800\n",
            "train batch: 9900\n",
            "train batch: 10000\n",
            "train batch: 10100\n",
            "train batch: 10200\n",
            "train batch: 10300\n",
            "train batch: 10400\n",
            "train batch: 10500\n",
            "train batch: 10600\n",
            "train batch: 10700\n",
            "train batch: 10800\n",
            "train batch: 10900\n",
            "train batch: 11000\n",
            "train batch: 11100\n",
            "train batch: 11200\n",
            "train batch: 11300\n",
            "train batch: 11400\n",
            "train batch: 11500\n",
            "train batch: 11600\n",
            "train batch: 11700\n",
            "train batch: 11800\n",
            "train batch: 11900\n",
            "train batch: 12000\n",
            "train batch: 12100\n",
            "train batch: 12200\n",
            "train batch: 12300\n",
            "train batch: 12400\n",
            "train batch: 12500\n",
            "train batch: 12600\n",
            "train batch: 12700\n",
            "train batch: 12800\n",
            "train batch: 12900\n",
            "train batch: 13000\n",
            "train batch: 13100\n",
            "train batch: 13200\n",
            "train batch: 13300\n",
            "train batch: 13400\n",
            "train batch: 13500\n",
            "train batch: 13600\n",
            "train batch: 13700\n",
            "train batch: 13800\n",
            "train batch: 13900\n",
            "train batch: 14000\n",
            "train batch: 14100\n",
            "train batch: 14200\n",
            "train batch: 14300\n",
            "train batch: 14400\n",
            "train batch: 14500\n",
            "train batch: 14600\n",
            "train batch: 14700\n",
            "train batch: 14800\n",
            "train batch: 14900\n",
            "train batch: 15000\n",
            "train batch: 15100\n",
            "train batch: 15200\n",
            "train batch: 15300\n",
            "train batch: 15400\n",
            "train batch: 15500\n",
            "train batch: 15600\n",
            "train batch: 15700\n",
            "train batch: 15800\n",
            "train batch: 15900\n",
            "train batch: 16000\n",
            "train batch: 16100\n",
            "train batch: 16200\n",
            "train batch: 16300\n",
            "train batch: 16400\n",
            "train batch: 16500\n",
            "train batch: 16600\n",
            "train batch: 16700\n",
            "train batch: 16800\n",
            "train batch: 16900\n",
            "train batch: 17000\n",
            "train batch: 17100\n",
            "train batch: 17200\n",
            "train batch: 17300\n",
            "train batch: 17400\n",
            "train batch: 17500\n",
            "train batch: 17600\n",
            "train batch: 17700\n",
            "train batch: 17800\n",
            "train batch: 17900\n",
            "train batch: 18000\n",
            "train batch: 18100\n",
            "train batch: 18200\n",
            "train batch: 18300\n",
            "train batch: 18400\n",
            "train batch: 18500\n",
            "train batch: 18600\n",
            "train batch: 18700\n",
            "train batch: 18800\n",
            "train batch: 18900\n",
            "train batch: 19000\n",
            "train batch: 19100\n",
            "train batch: 19200\n",
            "train batch: 19300\n",
            "train batch: 19400\n",
            "train batch: 19500\n",
            "train batch: 19600\n",
            "train batch: 19700\n",
            "train batch: 19800\n",
            "train batch: 19900\n",
            "train batch: 20000\n",
            "train batch: 20100\n",
            "train batch: 20200\n",
            "train batch: 20300\n",
            "train batch: 20400\n",
            "train batch: 20500\n",
            "train batch: 20600\n",
            "train batch: 20700\n",
            "train batch: 20800\n",
            "train batch: 20900\n",
            "train batch: 21000\n",
            "train batch: 21100\n",
            "train batch: 21200\n",
            "train batch: 21300\n",
            "train batch: 21400\n",
            "train batch: 21500\n",
            "train batch: 21600\n",
            "train batch: 21700\n",
            "train batch: 21800\n",
            "train batch: 21900\n",
            "train batch: 22000\n",
            "train batch: 22100\n",
            "train batch: 22200\n",
            "train batch: 22300\n",
            "train batch: 22400\n",
            "train batch: 22500\n",
            "train batch: 22600\n",
            "train batch: 22700\n",
            "train batch: 22800\n",
            "train batch: 22900\n",
            "train batch: 23000\n",
            "train batch: 23100\n",
            "train batch: 23200\n",
            "train batch: 23300\n",
            "train batch: 23400\n",
            "train batch: 23500\n",
            "train batch: 23600\n",
            "train batch: 23700\n",
            "train batch: 23800\n",
            "train batch: 23900\n",
            "train batch: 24000\n",
            "train batch: 24100\n",
            "train batch: 24200\n",
            "train batch: 24300\n",
            "train batch: 24400\n",
            "train batch: 24500\n",
            "train batch: 24600\n",
            "train batch: 24700\n",
            "train batch: 24800\n",
            "train batch: 24900\n",
            "train batch: 25000\n",
            "train batch: 25100\n",
            "train batch: 25200\n",
            "train batch: 25300\n",
            "train batch: 25400\n",
            "train batch: 25500\n",
            "train batch: 25600\n",
            "train batch: 25700\n",
            "train batch: 25800\n",
            "train batch: 25900\n",
            "train batch: 26000\n",
            "train batch: 26100\n",
            "train batch: 26200\n",
            "train batch: 26300\n",
            "train batch: 26400\n",
            "train batch: 26500\n",
            "train batch: 26600\n",
            "train batch: 26700\n",
            "train batch: 26800\n",
            "train batch: 26900\n",
            "train batch: 27000\n",
            "train batch: 27100\n",
            "train batch: 27200\n",
            "train batch: 27300\n",
            "train batch: 27400\n",
            "train batch: 27500\n",
            "train batch: 27600\n",
            "train batch: 27700\n",
            "train batch: 27800\n",
            "train batch: 27900\n",
            "train batch: 28000\n",
            "train batch: 28100\n",
            "train batch: 28200\n",
            "train batch: 28300\n",
            "train batch: 28400\n",
            "train batch: 28500\n",
            "train batch: 28600\n",
            "train batch: 28700\n",
            "train batch: 28800\n",
            "train batch: 28900\n",
            "train batch: 29000\n",
            "train batch: 29100\n",
            "train batch: 29200\n",
            "train batch: 29300\n",
            "train batch: 29400\n",
            "train batch: 29500\n",
            "train batch: 29600\n",
            "train batch: 29700\n",
            "train batch: 29800\n",
            "train batch: 29900\n",
            "train batch: 30000\n",
            "train batch: 30100\n",
            "train batch: 30200\n",
            "train batch: 30300\n",
            "train batch: 30400\n",
            "train batch: 30500\n",
            "train batch: 30600\n",
            "train batch: 30700\n",
            "train batch: 30800\n",
            "train batch: 30900\n",
            "train batch: 31000\n",
            "train batch: 31100\n",
            "train batch: 31200\n",
            "valuation - target sentence 100:  Australian diplomats played a key role in pushing for &quot; sustainable development goals &quot; to replace the MDGs , which expire in 2015 , ahead of the UN sustainable development summit that began in Rio de Janeiro overnight .             \n",
            "valuation - target sentence 100:  Im <unk> des UN ##AT##-##AT## <unk> für <unk> Entwicklung , der am <unk> in Rio de Janeiro <unk> , <unk> <unk> <unk> eine <unk> beim <unk> für „ <unk> <unk> “ , die die 2015 <unk> <unk> <unk> sollen . \n",
            "valuation - target sentence 200:  The introduction of a toll for cars would also only be a first step , in <unk> &apos;s eyes .                            \n",
            "valuation - target sentence 200:  Die <unk> einer <unk> für alle <unk> wäre aus <unk> <unk> <unk> nur ein <unk> <unk> . \n",
            "valuation - target sentence 300:  However , their repertoire also includes emotive <unk> and a full big band sound .                                                         \n",
            "valuation - target sentence 300:  Zu ihrem <unk> <unk> aber auch <unk> <unk> und ein <unk> Big ##AT##-##AT## Band ##AT##-##AT## Sound . \n",
            "valuation - target sentence 400:  &quot; We get to them first , before they can commit an act of violence &quot; .                                           \n",
            "valuation - target sentence 400:  &quot; Wir <unk> sie uns , noch <unk> sie ein <unk> <unk> können &quot; . \n",
            "valuation - target sentence 500:  And right at the front , on the stand , a single large flag with the text &quot; Stadium Ban Section &quot; blew in the wind .                              \n",
            "valuation - target sentence 500:  Und ganz <unk> auf der <unk> <unk> <unk> eine große <unk> mit <unk> &quot; <unk> <unk> &quot; . \n",
            "valuation - target sentence 600:  Reports in Australia said that in the meantime , she was <unk> at the resort area of Krabi in Southern Thailand .                              \n",
            "valuation - target sentence 600:  In <unk> <unk> war zu <unk> , dass sie in der <unk> im <unk> <unk> in <unk> Urlaub macht . \n",
            "valuation - target sentence 700:  Each of these weekend clinics provides a variety of medical care .                                    \n",
            "valuation - target sentence 700:  <unk> dieser <unk> bietet <unk> <unk> in einer <unk> von <unk> an . \n",
            "valuation - target sentence 800:  &quot; Most of these <unk> appeared at <unk> , and went on to greater success , to the point where they &apos;re too big to play a club , &quot; Hirsch said .                                                 \n",
            "valuation - target sentence 800:  „ Die meisten dieser <unk> <unk> bei <unk> an und wurden dann immer <unk> , bis sie <unk> zu bekannt waren , um in einem Club <unk> “ , <unk> <unk> . \n",
            "valuation - target sentence 900:  The projects are planned to start in <unk> and <unk> .                                                     \n",
            "valuation - target sentence 900:  Der Start wird <unk> <unk> und <unk> sein . \n",
            "valuation - target sentence 1000:  Girls who blossom early need reassurance that , even when it happens ahead of schedule , the process is a normal part of life .                             \n",
            "valuation - target sentence 1000:  <unk> , die <unk> <unk> , <unk> die <unk> , das der <unk> ein <unk> Teil des <unk> ist , selbst wenn er <unk> als <unk> <unk> . \n",
            "valuation - target sentence 1100:  You almost think that you can see the folds in the old man &apos;s neck , of which young fisherman <unk> speaks .                             \n",
            "valuation - target sentence 1100:  Man <unk> fast , die <unk> im <unk> des alten <unk> , von denen der <unk> <unk> <unk> , sehen zu können . \n",
            "valuation - target sentence 1200:  The meeting point for the one and a half hour tour is the Nuclear <unk> Museum , at 3 : 00 p.m.                            \n",
            "valuation - target sentence 1200:  <unk> zu der <unk> ­ <unk> <unk> ist um 15 Uhr am <unk> ##AT##-##AT## Museum . \n",
            "valuation - target sentence 1300:  In spite of his heart defect , <unk> could not be deterred from pursuing his dream .                            \n",
            "valuation - target sentence 1300:  <unk> ist trotz eines <unk> nicht von seinem <unk> <unk> . \n",
            "valuation - target sentence 1400:  But ever since Edward <unk> , the contractor turned <unk> , began releasing his treasure trove of US surveillance secrets , European governments and business leaders are no longer sure whether to take the director at his word .             \n",
            "valuation - target sentence 1400:  <unk> seit Edward <unk> , der zum <unk> <unk> <unk> Mitarbeiter , mit der <unk> <unk> <unk> an US ##AT##-##AT## <unk> <unk> <unk> hat , sind sich <unk> <unk> und <unk> nicht mehr sicher , ob sie den <unk> beim <unk> nehmen können . \n",
            "valuation - target sentence 1500:  According to the authorities , a ladder runs 20 metres underground to the actual entrance of the tunnel .                               \n",
            "valuation - target sentence 1500:  <unk> <unk> führt eine <unk> 20 Meter in die <unk> zum <unk> <unk> . \n",
            "valuation - target sentence 1600:  <unk> &apos;s blonde locks were made famous by her role in sexy TV show <unk> .                                          \n",
            "valuation - target sentence 1600:  <unk> <unk> <unk> sind seit ihrer Rolle in der <unk> <unk> <unk> . \n",
            "valuation - target sentence 1700:  Now , Airbus is appealing directly to the public ahead of the Dubai Airshow , where the <unk> is expected to dominate with more than 100 orders .                             \n",
            "valuation - target sentence 1700:  <unk> <unk> Airbus vor der Dubai <unk> , wo die <unk> mit über 100 <unk> <unk> das <unk> machen wird , direkt an die <unk> . \n",
            "valuation - target sentence 1800:  The scene was &quot; like something out of a movie &quot; .                                         \n",
            "valuation - target sentence 1800:  Die <unk> sei &quot; wie in einem Film &quot; <unk> . \n",
            "valuation - target sentence 1900:  Both <unk> and <unk> insist they want to avoid war .                                                                     \n",
            "valuation - target sentence 1900:  <unk> <unk> als auch <unk> <unk> , dass sie einen <unk> <unk> möchten . \n",
            "valuation - target sentence 2000:  In any case , a spokesperson for the Israeli Ministry of Defence said : &quot; We are not commenting on these reports &quot; .                    \n",
            "valuation - target sentence 2000:  Ein <unk> des <unk> <unk> <unk> <unk> : &quot; Wir <unk> diese <unk> nicht &quot; . \n",
            "valuation - target sentence 2100:  The rate of 3.1 per cent is indeed better than the previous year and is also better than in September , &quot; however , we had hoped for more , &quot; said Monika <unk> ##AT##-##AT## Bauer , acting branch manager of the Employment Agency in <unk> .       \n",
            "valuation - target sentence 2100:  <unk> sei die Quote mit <unk> Prozent besser als <unk> Jahr und auch besser als im September , &quot; wir <unk> uns aber mehr <unk> &quot; , <unk> <unk> <unk> ##AT##-##AT## Bauer , <unk> <unk> der <unk> für Arbeit in <unk> . \n",
            "valuation - target sentence 2200:  Rainer Schulze is an expert when it comes to the Cologne underworld .                                              \n",
            "valuation - target sentence 2200:  Rainer Schulze ist <unk> in <unk> <unk> <unk> . \n",
            "valuation - target sentence 2300:  When the fire service arrived , the flames were already bursting out of a window .                                           \n",
            "valuation - target sentence 2300:  Als die <unk> <unk> , <unk> die <unk> bereits aus einem <unk> . \n",
            "valuation - target sentence 2400:  Co ##AT##-##AT## operation with other like ##AT##-##AT## minded countries will be easier in a non ##AT##-##AT## federal Europe of the Nations .                                        \n",
            "valuation - target sentence 2400:  Die <unk> mit anderen , <unk> <unk> <unk> ist in einem nicht <unk> Europa der <unk> <unk> . \n",
            "valuation - target sentence 2500:  In addition , they have cleaned and widened the footpath , a press release announced .                                                         \n",
            "valuation - target sentence 2500:  <unk> <unk> und <unk> sie den <unk> , <unk> es in einer <unk> . \n",
            "valuation - target sentence 2600:  The FAA said it has already received plans from some airlines to expand the use of portable electronic devices on planes .                       \n",
            "valuation - target sentence 2600:  Die <unk> <unk> , sie habe bereits von einigen <unk> <unk> erhalten , die den Einsatz <unk> <unk> <unk> in <unk> <unk> möchten . \n",
            "valuation - target sentence 2700:  The ice cream harnesses the fluorescent properties of a jellyfish , synthesized by Chinese scientists                                     \n",
            "valuation - target sentence 2700:  Die <unk> <unk> die <unk> <unk> einer <unk> , die von <unk> <unk> <unk> <unk> werden \n",
            "Epoch: 9, Train loss: 0.263, Val loss: 23.869, blue score = 0.0079,Epoch time = 4248.536s\n",
            "start epoch: 10\n",
            "train batch: 100\n",
            "train batch: 200\n",
            "train batch: 300\n",
            "train batch: 400\n",
            "train batch: 500\n",
            "train batch: 600\n",
            "train batch: 700\n",
            "train batch: 800\n",
            "train batch: 900\n",
            "train batch: 1000\n",
            "train batch: 1100\n",
            "train batch: 1200\n",
            "train batch: 1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0zBOuAv7l1R"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dq3QzWW7l1R"
      },
      "outputs": [],
      "source": [
        "# print(translate(transformer, \"Ich lieb dich so sehr.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONgOuUscGtKd"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "scores = 0\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "num_sent = 0\n",
        "for inputs_batch, targets_batch in test_dataloader:\n",
        "  for i in range(inputs_batch.size(1)):\n",
        "    num_sent += 1\n",
        "    inputs, targets = inputs_batch[:,i:i+1], targets_batch[:,i:i+1]\n",
        "    \n",
        "    target_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(targets.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\").strip()\n",
        "\n",
        "    pred_sentence = translate(transformer, inputs)\n",
        "    if num_sent % 100 == 0:\n",
        "      print(f'testing - target sentence {num_sent}: {target_sentence}')\n",
        "      print(f'testing - target sentence {num_sent}: {pred_sentence}')\n",
        "\n",
        "    score = bleu_score([pred_sentence.split()], [[target_sentence.split()]])\n",
        "    scores += score \n",
        "    num_sent += 1\n",
        "\n",
        "print(f\"Test Bleu score: {scores / num_sent}\")\n",
        "\n",
        "# for src, tgt in val_dataloader:\n",
        "#     num_batch += 1\n",
        "#     src = src.to(DEVICE)\n",
        "#     tgt = tgt.to(DEVICE)\n",
        "\n",
        "#     tgt_input = tgt[:-1, :]\n",
        "\n",
        "#     src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "#     logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "#     tgt_out = tgt[1:, :]\n",
        "#     loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "#     losses += loss.item()\n",
        "\n",
        "# print(losses / num_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veckPeNm7l1S"
      },
      "source": [
        "References\n",
        "----------\n",
        "\n",
        "1. Attention is all you need paper.\n",
        "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "gcn_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}